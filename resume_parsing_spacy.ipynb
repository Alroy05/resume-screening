{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP958wY3f0i+y+mfoVPr6jL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alroy05/resume-screening/blob/master/resume_parsing_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JDThd3ghJ5g"
      },
      "outputs": [],
      "source": [
        "!pip install spacy-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "id": "hoZdc5HwkFls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/laxmimerit/CV-Parsing-using-Spacy-3.git"
      ],
      "metadata": {
        "id": "qXbprz5u31iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc993eae-1eb0-4162-892e-1698811f37ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CV-Parsing-using-Spacy-3'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 82 (delta 14), reused 74 (delta 14), pack-reused 6\u001b[K\n",
            "Receiving objects: 100% (82/82), 5.62 MiB | 28.19 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "import json"
      ],
      "metadata": {
        "id": "Z0WNuDOfke-h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data = json.load(open('/content/CV-Parsing-using-Spacy-3/data/training/train_data.json','r'))"
      ],
      "metadata": {
        "id": "xR4do02yn0Gv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cv_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2a4gotvoSDj",
        "outputId": "7e78b0a3-9460-4864-c77f-cd4f520e1f2a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /content/CV-Parsing-using-Spacy-3/data/training/base_config.cfg config.cfg"
      ],
      "metadata": {
        "id": "NO4RdzVGoa-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda71708-8411-4499-d316-714ec4d33227"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-02 01:33:10.773330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data"
      ],
      "metadata": {
        "id": "ZACawJTNpTca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_spacy_doc(file,data):\n",
        "  nlp = spacy.blank('en')\n",
        "  db = DocBin()\n",
        "\n",
        "  for text,annot in tqdm(data):\n",
        "    doc = nlp.make_doc(text)\n",
        "    annot = annot['entities']\n",
        "\n",
        "    ents = []\n",
        "    entity_indices = []\n",
        "\n",
        "    for start,end,label in annot:\n",
        "      skip_entity = False\n",
        "      for idx in range(start,end):\n",
        "        if idx in entity_indices:\n",
        "          skip_entity=True\n",
        "          break\n",
        "      if skip_entity==True:\n",
        "        continue\n",
        "\n",
        "      entity_indices = entity_indices + list(range(start,end))\n",
        "\n",
        "      try:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "      if span is None:\n",
        "        err_data = str([start, end]) + \"    \" + str(text) + \"\\n\"\n",
        "        file.write(err_data)\n",
        "\n",
        "      else:\n",
        "        ents.append(span)\n",
        "\n",
        "    try:\n",
        "      doc.ents = ents\n",
        "      db.add(doc)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  return db\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7NPeSG0YsWJs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(cv_data, test_size=1)"
      ],
      "metadata": {
        "id": "zmXJFDlfwUzu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train), len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gykTd7H-wo4Z",
        "outputId": "966f2f9c-f6d5-4220-bd4b-06d2fa795592"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(199, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('error.txt', 'w')\n",
        "\n",
        "db = get_spacy_doc(file, train)\n",
        "db.to_disk('train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file, test)\n",
        "db.to_disk('test_data.spacy')\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idhu11hmw6cd",
        "outputId": "ec832c16-54ff-43dd-e0d4-173ce82cd326"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 199/199 [00:01<00:00, 102.29it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 89.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(db.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny4A-UwgyHtE",
        "outputId": "dfdaca3b-8fbe-403f-de3a-5a01a973cb4f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/CV-Parsing-using-Spacy-3/data/training/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH9dpR9oycRQ",
        "outputId": "1f168d07-a495-406a-ffe3-5e327701e6da"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-02 01:34:26.208039: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 3.16MB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 9.74MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 5.99MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 10.5MB/s]\n",
            "Downloading model.safetensors: 100% 499M/499M [00:11<00:00, 44.9MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        2248.60   1606.20    0.00    0.00    0.00    0.00\n",
            "  2     200      274261.93  63651.43   28.57   42.86   21.43    0.29\n",
            "  5     400       33532.82  23730.06   58.06   52.94   64.29    0.58\n",
            "  8     600        7637.16  20636.71   75.00   66.67   85.71    0.75\n",
            " 11     800        8405.50  19078.89   76.92   83.33   71.43    0.77\n",
            " 13    1000        3259.02  16494.52   74.07   76.92   71.43    0.74\n",
            " 16    1200       13636.25  15842.97   66.67   62.50   71.43    0.67\n",
            " 19    1400        1543.84  15481.85   66.67  100.00   50.00    0.67\n",
            " 22    1600        3543.85  15013.74   80.00   75.00   85.71    0.80\n",
            " 25    1800       17382.20  14553.68   60.00   56.25   64.29    0.60\n",
            " 27    2000        2097.22  12267.39   80.00   75.00   85.71    0.80\n",
            " 30    2200       20681.02  12934.85   58.33   70.00   50.00    0.58\n",
            " 33    2400         646.44  12163.89   64.00   72.73   57.14    0.64\n",
            " 36    2600        2090.97  11639.37   68.97   66.67   71.43    0.69\n",
            " 38    2800         923.42  10187.68   62.50   55.56   71.43    0.63\n",
            "\n",
            "\u001b[31mAborted.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('/content/output/model-best')"
      ],
      "metadata": {
        "id": "X_wO4Z7yL71U"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('my name is Zane Falcao. I worked at Microsoft. I have 3 years of experience')\n",
        "\n",
        "for ent in doc.ents:\n",
        "   print(ent.text, \" ->>>>>\", ent.label_)"
      ],
      "metadata": {
        "id": "5D75trAHM-Nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b60453-6331-4b86-f3c6-eb5231ce4f19"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zane Falcao  ->>>>> Name\n",
            "Microsoft  ->>>>> Companies worked at\n",
            "3 years  ->>>>> Years of Experience\n"
          ]
        }
      ]
    }
  ]
}